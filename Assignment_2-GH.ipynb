{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "## First draft\n",
    "### Youssef Ragab, Ting Huang, Garrett Hastings\n",
    "\n",
    "\n",
    "**1. Loading in the required libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import h2o \n",
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator \n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
    "from h2o.grid.grid_search import H2OGridSearch \n",
    "from h2o.estimators.xgboost import H2OXGBoostEstimator\n",
    "from h2o.estimators.stackedensemble import H2OStackedEnsembleEstimator \n",
    "from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n",
    "from h2o.frame import H2OFrame      \n",
    "import itertools       \n",
    "import matplotlib.pyplot as plt        \n",
    "import xgboost as xgb \n",
    "import time \n",
    "import seaborn as sns \n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 . connected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>48 mins 49 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>America/New_York</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.32.1.3</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>16 days </td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_ghast_arvo6t</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>5.948 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O_API_Extensions:</td>\n",
       "<td>Amazon S3, Algos, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.7.3 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ---------------------------------------------------------\n",
       "H2O_cluster_uptime:         48 mins 49 secs\n",
       "H2O_cluster_timezone:       America/New_York\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.32.1.3\n",
       "H2O_cluster_version_age:    16 days\n",
       "H2O_cluster_name:           H2O_from_python_ghast_arvo6t\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    5.948 Gb\n",
       "H2O_cluster_total_cores:    8\n",
       "H2O_cluster_allowed_cores:  4\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://localhost:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "H2O_API_Extensions:         Amazon S3, Algos, AutoML, Core V3, TargetEncoder, Core V4\n",
       "Python_version:             3.7.3 final\n",
       "--------------------------  ---------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Seed = 2021\n",
    "\n",
    "nthreads = 4\n",
    "\n",
    "h2o.init(max_mem_size= '6G', nthreads= nthreads)\n",
    "h2o.remove_all()\n",
    "h2o.no_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start global timer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('hmda_train_preprocessed.csv')\n",
    "test = pd.read_csv('hmda_test_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['row_id', 'black', 'asian', 'white', 'amind', 'hipac', 'hispanic',\n",
       "       'non_hispanic', 'male', 'female', 'agegte62', 'agelt62', 'term_360',\n",
       "       'conforming', 'debt_to_income_ratio_missing', 'loan_amount_std',\n",
       "       'loan_to_value_ratio_std', 'no_intro_rate_period_std',\n",
       "       'intro_rate_period_std', 'property_value_std', 'income_std',\n",
       "       'debt_to_income_ratio_std', 'high_priced'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['row_id', 'black', 'asian', 'white', 'amind', 'hipac', 'hispanic',\n",
       "       'non_hispanic', 'male', 'female', 'agegte62', 'agelt62', 'term_360',\n",
       "       'conforming', 'debt_to_income_ratio_missing', 'loan_amount_std',\n",
       "       'loan_to_value_ratio_std', 'no_intro_rate_period_std',\n",
       "       'intro_rate_period_std', 'property_value_std', 'income_std',\n",
       "       'debt_to_income_ratio_std'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the datasets loaded, we can proceed and remove the columns that we don't need from the training set. Based on the assignment prompt, we are going to keep the following variables:\n",
    "* high_priced\n",
    "* conforming \n",
    "* debt_to_income_ratio_std\n",
    "* debt_to_income_ratio_missing\n",
    "* income_std \n",
    "* loan_amount_std\n",
    "* intro_rate_period_std\n",
    "* loan_to_value_ratio_std\n",
    "* no_intro_rate_period_std\n",
    "* property_value_std\n",
    "* term_360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_name = ['conforming', 'debt_to_income_ratio_std', 'debt_to_income_ratio_missing', 'income_std', 'loan_amount_std', 'intro_rate_period_std', 'loan_to_value_ratio_std', 'no_intro_rate_period_std', 'property_value_std', 'term_360']\n",
    "y_name = 'high_priced'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing this so that 'row_id' is in the dataframe later.\n",
    "\n",
    "#train = train[x_name + [y_name]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have only kept the columns that we need from the training set, we can do the same for the testing or validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing this so that 'row_id' is in the dataframe later.\n",
    "\n",
    "#test = test[x_name]\n",
    "#test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration:\n",
    "\n",
    "**Histograms:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_ = train.hist(bins = 50, figsize= (15,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_ = sns.heatmap(train.corr(),\n",
    "                #xticklabels= train.columns.values,\n",
    "                #yticklabels= train.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the testing set does not have the y column, there is no need to add it, and we are left with the same variables as the training set. \n",
    "\n",
    "We can move ahead to training the model. \n",
    "\n",
    "### Fitting interpretable models: \n",
    "\n",
    "**Splitting data into training and validation subsets:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data rows = 112085, columns = 23\n",
      "Train data rows = 48253, columns = 23\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(Seed)\n",
    "\n",
    "split_ratio = 0.7\n",
    "\n",
    "split = np.random.rand(len(train)) < split_ratio\n",
    "\n",
    "training = train[split]\n",
    "validation = train[~split]\n",
    "\n",
    "print('Train data rows = %d, columns = %d' % (training.shape[0], training.shape[1]))\n",
    "print('Train data rows = %d, columns = %d' % (validation.shape[0], validation.shape[1])) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net\n",
    "\n",
    "**Defining wrapper function for hyperparameter grid search:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glm_grid(x_name, y_name, htrain, hvalid, seed):\n",
    "\n",
    "    # Creating alpha options to set L2:\n",
    "    alpha_options = [0.1, 0.25, 0.5, 0.99]\n",
    "    \n",
    "    # Defining serch criteria\n",
    "    hyperparameters = {'alpha': alpha_options}\n",
    "\n",
    "    # Initializing grid \n",
    "    grid = H2OGridSearch(\n",
    "        H2OGeneralizedLinearEstimator(family = 'binomial',\n",
    "                                      lambda_search = True,\n",
    "                                      seed = seed),\n",
    "        hyper_params = hyperparameters)\n",
    "\n",
    "    # Executing training with grid search\n",
    "    grid.train(y = y_name,\n",
    "               x = x_name,\n",
    "               training_frame = htrain, \n",
    "               validation_frame = hvalid,\n",
    "               seed = seed)\n",
    "\n",
    "    # Choosing the best model from the grid seach \n",
    "    best_model = grid.get_grid()[0]\n",
    "    del grid \n",
    "\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting elastic net:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic net GLM training completed in 45.11 s.\n"
     ]
    }
   ],
   "source": [
    "# Start timer \n",
    "glm_tic = time.time()\n",
    "\n",
    "# Converting data into H2O Frame \n",
    "htrain = h2o.H2OFrame(training)\n",
    "hvalid = h2o.H2OFrame(validation)\n",
    "\n",
    "# Training with grid search \n",
    "best_glm = glm_grid(x_name, y_name, htrain, hvalid, Seed)\n",
    "\n",
    "# End timer \n",
    "glm_toc = time.time() - glm_tic\n",
    "print('Elastic net GLM training completed in %.2f s.' % glm_toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best AUC assessment:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC: 0.7563.\n"
     ]
    }
   ],
   "source": [
    "print('Validation AUC: %.4f.' % best_glm.auc(valid = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Writing submission files:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_glm_submit = best_glm.predict(h2o.H2OFrame(test)).as_data_frame()\n",
    "best_glm_submit.drop(['predict', 'p0'], axis = 1, inplace = True)\n",
    "best_glm_submit.columns = ['phat']\n",
    "best_glm_submit.to_csv('best_glm_' + str(datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\") + '.csv'), \n",
    "                        index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.143552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.081619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.128237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.131587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       phat\n",
       "0  0.143552\n",
       "1  0.081619\n",
       "2  0.128237\n",
       "3  0.005958\n",
       "4  0.131587"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_glm_submit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW CODE\n",
    "\n",
    "#### Utility function for selecting percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>black</th>\n",
       "      <th>asian</th>\n",
       "      <th>white</th>\n",
       "      <th>amind</th>\n",
       "      <th>hipac</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>non_hispanic</th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "      <th>...</th>\n",
       "      <th>debt_to_income_ratio_missing</th>\n",
       "      <th>loan_amount_std</th>\n",
       "      <th>loan_to_value_ratio_std</th>\n",
       "      <th>no_intro_rate_period_std</th>\n",
       "      <th>intro_rate_period_std</th>\n",
       "      <th>property_value_std</th>\n",
       "      <th>income_std</th>\n",
       "      <th>debt_to_income_ratio_std</th>\n",
       "      <th>high_priced</th>\n",
       "      <th>phat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.118642</td>\n",
       "      <td>0.268727</td>\n",
       "      <td>0.244394</td>\n",
       "      <td>-0.215304</td>\n",
       "      <td>-0.227585</td>\n",
       "      <td>-0.018133</td>\n",
       "      <td>-0.425131</td>\n",
       "      <td>0</td>\n",
       "      <td>0.143552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.602338</td>\n",
       "      <td>0.552520</td>\n",
       "      <td>0.244394</td>\n",
       "      <td>-0.215304</td>\n",
       "      <td>-0.628437</td>\n",
       "      <td>-0.038228</td>\n",
       "      <td>0.763191</td>\n",
       "      <td>0</td>\n",
       "      <td>0.081619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.734255</td>\n",
       "      <td>0.552520</td>\n",
       "      <td>0.244394</td>\n",
       "      <td>-0.215304</td>\n",
       "      <td>-0.720941</td>\n",
       "      <td>-0.039614</td>\n",
       "      <td>0.488963</td>\n",
       "      <td>0</td>\n",
       "      <td>0.128237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.332446</td>\n",
       "      <td>-0.261719</td>\n",
       "      <td>0.244394</td>\n",
       "      <td>-0.215304</td>\n",
       "      <td>1.190815</td>\n",
       "      <td>0.075764</td>\n",
       "      <td>-1.156406</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.514393</td>\n",
       "      <td>0.333922</td>\n",
       "      <td>0.244394</td>\n",
       "      <td>-0.215304</td>\n",
       "      <td>-0.535932</td>\n",
       "      <td>-0.024369</td>\n",
       "      <td>-0.425131</td>\n",
       "      <td>1</td>\n",
       "      <td>0.131587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id  black  asian  white  amind  hipac  hispanic  non_hispanic  male  \\\n",
       "0       1    0.0    0.0    1.0    0.0    0.0       0.0           1.0   NaN   \n",
       "1       4    0.0    0.0    1.0    0.0    0.0       1.0           0.0   NaN   \n",
       "2       7    0.0    0.0    1.0    0.0    0.0       0.0           1.0   1.0   \n",
       "3       9    0.0    0.0    1.0    0.0    0.0       0.0           1.0   NaN   \n",
       "4      12    1.0    0.0    0.0    0.0    0.0       0.0           1.0   NaN   \n",
       "\n",
       "   female  ...  debt_to_income_ratio_missing  loan_amount_std  \\\n",
       "0     NaN  ...                             0        -0.118642   \n",
       "1     NaN  ...                             0        -0.602338   \n",
       "2     0.0  ...                             0        -0.734255   \n",
       "3     NaN  ...                             0         1.332446   \n",
       "4     NaN  ...                             0        -0.514393   \n",
       "\n",
       "   loan_to_value_ratio_std  no_intro_rate_period_std  intro_rate_period_std  \\\n",
       "0                 0.268727                  0.244394              -0.215304   \n",
       "1                 0.552520                  0.244394              -0.215304   \n",
       "2                 0.552520                  0.244394              -0.215304   \n",
       "3                -0.261719                  0.244394              -0.215304   \n",
       "4                 0.333922                  0.244394              -0.215304   \n",
       "\n",
       "   property_value_std  income_std  debt_to_income_ratio_std  high_priced  \\\n",
       "0           -0.227585   -0.018133                 -0.425131            0   \n",
       "1           -0.628437   -0.038228                  0.763191            0   \n",
       "2           -0.720941   -0.039614                  0.488963            0   \n",
       "3            1.190815    0.075764                 -1.156406            0   \n",
       "4           -0.535932   -0.024369                 -0.425131            1   \n",
       "\n",
       "       phat  \n",
       "0  0.143552  \n",
       "1  0.081619  \n",
       "2  0.128237  \n",
       "3  0.005958  \n",
       "4  0.131587  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_glm_submit = pd.concat([validation.reset_index(drop=True), best_glm_submit], axis=1)\n",
    "best_glm_submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percentile_dict(yhat_name, frame, id_):\n",
    "\n",
    "    \"\"\" Returns the percentiles of a column, yhat_name, as the indices based on\n",
    "        another column id_.\n",
    "        :param yhat_name: Name of column in frame in which to find percentiles.\n",
    "        :param frame: Pandas frame.\n",
    "        :param id_: Validation Pandas frame containing yhat and id_.\n",
    "        :return: Dictionary of percentile values and index column values.\n",
    "    \"\"\"\n",
    "\n",
    "    # create a copy of frame and sort it by yhat\n",
    "    sort_df = frame.copy(deep=True)\n",
    "    sort_df.sort_values(yhat_name, inplace=True)\n",
    "    sort_df.reset_index(inplace=True)\n",
    "\n",
    "    # find top and bottom percentiles\n",
    "    percentiles_dict = {0: sort_df.loc[0, id_], 99: sort_df.loc[sort_df.shape[0] - 1, id_]}\n",
    "\n",
    "    # find 10th-90th percentiles\n",
    "    inc = sort_df.shape[0] // 10\n",
    "    for i in range(1, 10):\n",
    "        percentiles_dict[i * 10] = sort_df.loc[i * inc, id_]\n",
    "\n",
    "    return percentiles_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select percentiles from predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 15340\n",
      "10: 65757\n",
      "20: 42634\n",
      "30: 49003\n",
      "40: 20879\n",
      "50: 80111\n",
      "60: 96502\n",
      "70: 112762\n",
      "80: 128906\n",
      "90: 144457\n",
      "99: 160335\n"
     ]
    }
   ],
   "source": [
    "best_glm_percentiles = get_percentile_dict('phat', best_glm_submit, 'row_id')\n",
    "for key in sorted(best_glm_percentiles.keys()):\n",
    "    print(str(key) + ': ' + str(best_glm_percentiles[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect global feature importance info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use coefficients as relative global feature importance\n",
    "global_feat_imp = pd.DataFrame.from_dict(best_glm.coef(), columns=['GLM Importance'], orient='index')\n",
    "global_feat_imp.drop('Intercept', inplace=True)\n",
    "global_feat_imp['GLM Importance'] = np.abs(global_feat_imp['GLM Importance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect local feature importance info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use (coefficient * value) as local feature importance\n",
    "\n",
    "# init frame to store local contribs\n",
    "local_fi_dict = {10: pd.DataFrame(columns = ['GLM Contribution'], index=x_name),\n",
    "                 50: pd.DataFrame(columns = ['GLM Contribution'], index=x_name),\n",
    "                 90: pd.DataFrame(columns = ['GLM Contribution'], index=x_name)}\n",
    "\n",
    "# get (coefficient * value) at three percentiles of phat\n",
    "for name in x_name:\n",
    "    for percentile in [10, 50, 90]:\n",
    "    \n",
    "        # local contributions = beta_j * x_i,j\n",
    "        local_fi_dict[percentile].loc[name, 'GLM Contribution'] =\\\n",
    "            best_glm.coef()[name] *\\\n",
    "            validation[validation['row_id'] == int(best_glm_percentiles[percentile])][name].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GLM Contribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>conforming</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>debt_to_income_ratio_std</th>\n",
       "      <td>0.131437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>debt_to_income_ratio_missing</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>income_std</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loan_amount_std</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intro_rate_period_std</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loan_to_value_ratio_std</th>\n",
       "      <td>-0.0500465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_intro_rate_period_std</th>\n",
       "      <td>-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_value_std</th>\n",
       "      <td>-2.20074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_360</th>\n",
       "      <td>0.222036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             GLM Contribution\n",
       "conforming                                  0\n",
       "debt_to_income_ratio_std             0.131437\n",
       "debt_to_income_ratio_missing                0\n",
       "income_std                                  0\n",
       "loan_amount_std                             0\n",
       "intro_rate_period_std                       0\n",
       "loan_to_value_ratio_std            -0.0500465\n",
       "no_intro_rate_period_std                   -0\n",
       "property_value_std                   -2.20074\n",
       "term_360                             0.222036"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#local_fi_dict[10]\n",
    "local_fi_dict[50]\n",
    "#local_fi_dict[90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monotonic XGBoost\n",
    "**Define utility function for random grid search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_grid(dtrain, dvalid, mono_constraints=None, gs_params=None, n_models=None,\n",
    "             ntree=None, early_stopping_rounds=None, verbose=False, seed=None):\n",
    "    \n",
    "\n",
    "    # cartesian product of gs_params\n",
    "    keys, values = zip(*gs_params.items())\n",
    "    experiments = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    # preserve exact reproducibility for this function\n",
    "    np.random.seed(Seed) \n",
    "    \n",
    "    # select randomly from cartesian product space\n",
    "    selected_experiments = np.random.choice(len(experiments), n_models)\n",
    "\n",
    "    # set global params for objective,  etc.\n",
    "    params = {'booster': 'gbtree',\n",
    "              'eval_metric': 'auc',\n",
    "              'nthread': nthreads,\n",
    "              'objective': 'binary:logistic',\n",
    "              'seed': Seed}\n",
    "\n",
    "    # init grid search loop\n",
    "    best_candidate = None\n",
    "    best_score = 0\n",
    "\n",
    "    # grid search loop\n",
    "    for i, exp in enumerate(selected_experiments):\n",
    "\n",
    "        params.update(experiments[exp])  # override global params with current grid run params\n",
    "\n",
    "        print('Grid search run %d/%d:' % (int(i + 1), int(n_models)))\n",
    "        print('Training with parameters:', params)\n",
    "\n",
    "        # train on current params\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "        \n",
    "        if mono_constraints is not None:\n",
    "            params['monotone_constraints'] = mono_constraints\n",
    "        \n",
    "        candidate = xgb.train(params,\n",
    "                              dtrain,\n",
    "                              ntree,\n",
    "                              early_stopping_rounds=early_stopping_rounds,\n",
    "                              evals=watchlist,\n",
    "                              verbose_eval=verbose)    \n",
    "\n",
    "        # determine if current model is better than previous best\n",
    "        if candidate.best_score > best_score:\n",
    "            best_candidate = candidate\n",
    "            best_score = candidate.best_score\n",
    "            print('Grid search new best score discovered at iteration %d/%d: %.4f.' %\n",
    "                             (int(i + 1), int(n_models), candidate.best_score))\n",
    "\n",
    "        print('---------- ----------')\n",
    "            \n",
    "    return best_candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit monotonic XGBoost with random grid search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search run 1/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.3, 'colsample_bylevel': 0.5, 'eta': 0.005, 'max_depth': 7, 'reg_alpha': 0.005, 'reg_lambda': 0.005, 'subsample': 0.5, 'min_child_weight': 5, 'gamma': 0.1}\n",
      "Grid search new best score discovered at iteration 1/50: 0.7708.\n",
      "---------- ----------\n",
      "Grid search run 2/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.3, 'colsample_bylevel': 0.3, 'eta': 0.05, 'max_depth': 5, 'reg_alpha': 0.05, 'reg_lambda': 0.05, 'subsample': 0.5, 'min_child_weight': 10, 'gamma': 0.4, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "Grid search new best score discovered at iteration 2/50: 0.7875.\n",
      "---------- ----------\n",
      "Grid search run 3/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.5, 'colsample_bylevel': 0.3, 'eta': 0.005, 'max_depth': 5, 'reg_alpha': 0.005, 'reg_lambda': 0.05, 'subsample': 0.7, 'min_child_weight': 5, 'gamma': 0.3, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 4/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.5, 'colsample_bylevel': 0.7, 'eta': 0.005, 'max_depth': 7, 'reg_alpha': 0.05, 'reg_lambda': 0.05, 'subsample': 0.7, 'min_child_weight': 10, 'gamma': 0.4, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 5/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.5, 'colsample_bylevel': 0.3, 'eta': 0.05, 'max_depth': 3, 'reg_alpha': 0.0005, 'reg_lambda': 0.005, 'subsample': 0.5, 'min_child_weight': 10, 'gamma': 0.4, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 6/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.5, 'colsample_bylevel': 0.7, 'eta': 0.05, 'max_depth': 7, 'reg_alpha': 0.05, 'reg_lambda': 0.0005, 'subsample': 0.9, 'min_child_weight': 10, 'gamma': 0.2, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "Grid search new best score discovered at iteration 6/50: 0.7898.\n",
      "---------- ----------\n",
      "Grid search run 7/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.5, 'colsample_bylevel': 0.3, 'eta': 0.5, 'max_depth': 7, 'reg_alpha': 0.005, 'reg_lambda': 0.0005, 'subsample': 0.3, 'min_child_weight': 5, 'gamma': 0.3, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 8/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.5, 'colsample_bylevel': 0.9, 'eta': 0.05, 'max_depth': 3, 'reg_alpha': 0.005, 'reg_lambda': 0.05, 'subsample': 0.3, 'min_child_weight': 1, 'gamma': 0.2, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 9/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.3, 'colsample_bylevel': 0.3, 'eta': 0.5, 'max_depth': 7, 'reg_alpha': 0.005, 'reg_lambda': 0.05, 'subsample': 0.5, 'min_child_weight': 5, 'gamma': 0.1, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 10/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.5, 'colsample_bylevel': 0.9, 'eta': 0.005, 'max_depth': 5, 'reg_alpha': 0.0005, 'reg_lambda': 0.0005, 'subsample': 0.3, 'min_child_weight': 5, 'gamma': 0.2, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 11/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.7, 'colsample_bylevel': 0.3, 'eta': 0.05, 'max_depth': 5, 'reg_alpha': 0.0005, 'reg_lambda': 0.05, 'subsample': 0.9, 'min_child_weight': 10, 'gamma': 0.2, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "Grid search new best score discovered at iteration 11/50: 0.7900.\n",
      "---------- ----------\n",
      "Grid search run 12/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.5, 'colsample_bylevel': 0.9, 'eta': 0.5, 'max_depth': 7, 'reg_alpha': 0.0005, 'reg_lambda': 0.0005, 'subsample': 0.5, 'min_child_weight': 10, 'gamma': 0.4, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 13/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.7, 'colsample_bylevel': 0.5, 'eta': 0.05, 'max_depth': 7, 'reg_alpha': 0.0005, 'reg_lambda': 0.0005, 'subsample': 0.5, 'min_child_weight': 5, 'gamma': 0.0, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "Grid search new best score discovered at iteration 13/50: 0.7907.\n",
      "---------- ----------\n",
      "Grid search run 14/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.5, 'colsample_bylevel': 0.7, 'eta': 0.5, 'max_depth': 3, 'reg_alpha': 0.0005, 'reg_lambda': 0.0005, 'subsample': 0.9, 'min_child_weight': 5, 'gamma': 0.0, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 15/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.3, 'colsample_bylevel': 0.7, 'eta': 0.05, 'max_depth': 3, 'reg_alpha': 0.05, 'reg_lambda': 0.0005, 'subsample': 0.9, 'min_child_weight': 1, 'gamma': 0.2, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 16/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.5, 'colsample_bylevel': 0.9, 'eta': 0.05, 'max_depth': 7, 'reg_alpha': 0.005, 'reg_lambda': 0.0005, 'subsample': 0.5, 'min_child_weight': 10, 'gamma': 0.2, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 17/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.5, 'colsample_bylevel': 0.5, 'eta': 0.05, 'max_depth': 7, 'reg_alpha': 0.005, 'reg_lambda': 0.005, 'subsample': 0.7, 'min_child_weight': 10, 'gamma': 0.0, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 18/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.3, 'colsample_bylevel': 0.7, 'eta': 0.5, 'max_depth': 5, 'reg_alpha': 0.05, 'reg_lambda': 0.05, 'subsample': 0.5, 'min_child_weight': 5, 'gamma': 0.2, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 19/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.7, 'colsample_bylevel': 0.9, 'eta': 0.005, 'max_depth': 3, 'reg_alpha': 0.0005, 'reg_lambda': 0.05, 'subsample': 0.9, 'min_child_weight': 1, 'gamma': 0.2, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 20/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.5, 'colsample_bylevel': 0.5, 'eta': 0.05, 'max_depth': 3, 'reg_alpha': 0.0005, 'reg_lambda': 0.005, 'subsample': 0.9, 'min_child_weight': 10, 'gamma': 0.1, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 21/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.5, 'colsample_bylevel': 0.7, 'eta': 0.005, 'max_depth': 5, 'reg_alpha': 0.005, 'reg_lambda': 0.05, 'subsample': 0.3, 'min_child_weight': 1, 'gamma': 0.1, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 22/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.9, 'colsample_bylevel': 0.7, 'eta': 0.005, 'max_depth': 5, 'reg_alpha': 0.05, 'reg_lambda': 0.005, 'subsample': 0.7, 'min_child_weight': 1, 'gamma': 0.0, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 23/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.3, 'colsample_bylevel': 0.5, 'eta': 0.5, 'max_depth': 3, 'reg_alpha': 0.005, 'reg_lambda': 0.005, 'subsample': 0.5, 'min_child_weight': 5, 'gamma': 0.1, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 24/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.5, 'colsample_bylevel': 0.9, 'eta': 0.5, 'max_depth': 7, 'reg_alpha': 0.05, 'reg_lambda': 0.005, 'subsample': 0.7, 'min_child_weight': 1, 'gamma': 0.4, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 25/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.9, 'colsample_bylevel': 0.7, 'eta': 0.5, 'max_depth': 5, 'reg_alpha': 0.005, 'reg_lambda': 0.0005, 'subsample': 0.9, 'min_child_weight': 1, 'gamma': 0.0, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "Grid search new best score discovered at iteration 25/50: 0.7913.\n",
      "---------- ----------\n",
      "Grid search run 26/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.3, 'colsample_bylevel': 0.5, 'eta': 0.05, 'max_depth': 7, 'reg_alpha': 0.05, 'reg_lambda': 0.05, 'subsample': 0.3, 'min_child_weight': 1, 'gamma': 0.2, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 27/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.7, 'colsample_bylevel': 0.3, 'eta': 0.05, 'max_depth': 7, 'reg_alpha': 0.05, 'reg_lambda': 0.05, 'subsample': 0.3, 'min_child_weight': 5, 'gamma': 0.1, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 28/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.5, 'colsample_bylevel': 0.9, 'eta': 0.005, 'max_depth': 3, 'reg_alpha': 0.05, 'reg_lambda': 0.05, 'subsample': 0.5, 'min_child_weight': 5, 'gamma': 0.2, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 29/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.5, 'colsample_bylevel': 0.7, 'eta': 0.5, 'max_depth': 5, 'reg_alpha': 0.05, 'reg_lambda': 0.0005, 'subsample': 0.5, 'min_child_weight': 5, 'gamma': 0.0, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 30/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.7, 'colsample_bylevel': 0.7, 'eta': 0.005, 'max_depth': 3, 'reg_alpha': 0.05, 'reg_lambda': 0.05, 'subsample': 0.9, 'min_child_weight': 5, 'gamma': 0.0, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 31/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.5, 'colsample_bylevel': 0.9, 'eta': 0.5, 'max_depth': 7, 'reg_alpha': 0.05, 'reg_lambda': 0.05, 'subsample': 0.7, 'min_child_weight': 10, 'gamma': 0.0, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 32/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.3, 'colsample_bylevel': 0.5, 'eta': 0.05, 'max_depth': 3, 'reg_alpha': 0.05, 'reg_lambda': 0.05, 'subsample': 0.3, 'min_child_weight': 5, 'gamma': 0.0, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 33/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.7, 'colsample_bylevel': 0.9, 'eta': 0.005, 'max_depth': 7, 'reg_alpha': 0.0005, 'reg_lambda': 0.005, 'subsample': 0.5, 'min_child_weight': 1, 'gamma': 0.2, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 34/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.5, 'colsample_bylevel': 0.5, 'eta': 0.005, 'max_depth': 7, 'reg_alpha': 0.005, 'reg_lambda': 0.0005, 'subsample': 0.9, 'min_child_weight': 1, 'gamma': 0.2, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 35/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.5, 'colsample_bylevel': 0.3, 'eta': 0.5, 'max_depth': 3, 'reg_alpha': 0.05, 'reg_lambda': 0.0005, 'subsample': 0.5, 'min_child_weight': 10, 'gamma': 0.2, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 36/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.7, 'colsample_bylevel': 0.5, 'eta': 0.005, 'max_depth': 7, 'reg_alpha': 0.005, 'reg_lambda': 0.05, 'subsample': 0.9, 'min_child_weight': 10, 'gamma': 0.0, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 37/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.3, 'colsample_bylevel': 0.5, 'eta': 0.5, 'max_depth': 3, 'reg_alpha': 0.05, 'reg_lambda': 0.05, 'subsample': 0.9, 'min_child_weight': 10, 'gamma': 0.4, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 38/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.5, 'colsample_bylevel': 0.3, 'eta': 0.5, 'max_depth': 5, 'reg_alpha': 0.0005, 'reg_lambda': 0.005, 'subsample': 0.5, 'min_child_weight': 10, 'gamma': 0.0, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 39/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.3, 'colsample_bylevel': 0.9, 'eta': 0.005, 'max_depth': 7, 'reg_alpha': 0.005, 'reg_lambda': 0.05, 'subsample': 0.3, 'min_child_weight': 10, 'gamma': 0.2, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 40/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.7, 'colsample_bylevel': 0.7, 'eta': 0.005, 'max_depth': 3, 'reg_alpha': 0.0005, 'reg_lambda': 0.0005, 'subsample': 0.5, 'min_child_weight': 1, 'gamma': 0.2, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 41/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.9, 'colsample_bylevel': 0.9, 'eta': 0.05, 'max_depth': 5, 'reg_alpha': 0.0005, 'reg_lambda': 0.0005, 'subsample': 0.9, 'min_child_weight': 5, 'gamma': 0.1, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 42/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.7, 'colsample_bylevel': 0.3, 'eta': 0.5, 'max_depth': 7, 'reg_alpha': 0.005, 'reg_lambda': 0.005, 'subsample': 0.5, 'min_child_weight': 5, 'gamma': 0.3, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 43/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.9, 'colsample_bylevel': 0.5, 'eta': 0.5, 'max_depth': 3, 'reg_alpha': 0.0005, 'reg_lambda': 0.005, 'subsample': 0.9, 'min_child_weight': 10, 'gamma': 0.0, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 44/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.3, 'colsample_bylevel': 0.3, 'eta': 0.5, 'max_depth': 7, 'reg_alpha': 0.05, 'reg_lambda': 0.0005, 'subsample': 0.3, 'min_child_weight': 10, 'gamma': 0.2, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n",
      "---------- ----------\n",
      "Grid search run 45/50:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 2021, 'colsample_bytree': 0.3, 'colsample_bylevel': 0.3, 'eta': 0.05, 'max_depth': 7, 'reg_alpha': 0.05, 'reg_lambda': 0.05, 'subsample': 0.9, 'min_child_weight': 1, 'gamma': 0.2, 'monotone_constraints': (1, 1, 1, -1, -1, -1, 1, 1, -1, 1)}\n"
     ]
    }
   ],
   "source": [
    "# dictionary of hyperparameter value lists for grid search\n",
    "gs_params = {'colsample_bytree': [0.3, 0.5, 0.7, 0.9],\n",
    "             'colsample_bylevel': [0.3, 0.5, 0.7, 0.9],\n",
    "             'eta': [0.005, 0.05, 0.5],\n",
    "             'max_depth': [3, 5, 7],\n",
    "             'reg_alpha': [0.0005, 0.005, 0.05],\n",
    "             'reg_lambda': [0.0005, 0.005, 0.05],\n",
    "             'subsample': [0.3, 0.5, 0.7, 0.9],\n",
    "             'min_child_weight': [1, 5, 10], \n",
    "             'gamma': [0.0, 0.1, 0.2 , 0.3, 0.4]}\n",
    "\n",
    "# define monotonicity constraints\n",
    "mono_constraints = tuple([int(i) for i in np.sign(train[x_name + [y_name]].corr()[y_name].values[:-1])])\n",
    "\n",
    "# start local timer\n",
    "mxgb_tic = time.time()\n",
    "\n",
    "# Convert data to SVMLight format\n",
    "dtrain = xgb.DMatrix(training[x_name], training[y_name])\n",
    "dvalid = xgb.DMatrix(validation[x_name], validation[y_name])\n",
    "\n",
    "# Monotonic XGBoost grid search\n",
    "best_mxgb = xgb_grid(dtrain, dvalid, gs_params=gs_params, n_models=50, ntree=1000, early_stopping_rounds=100, \n",
    "                     mono_constraints=mono_constraints, seed=Seed)\n",
    "\n",
    "# end local timer\n",
    "mxgb_toc = time.time() - mxgb_tic\n",
    "print('Monotonic GBM training completed in %.2f s.' % (mxgb_toc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic AUC assessment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Validation AUC: %.4f.' % best_mxgb.best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write submission file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest = xgb.DMatrix(test[x_name])\n",
    "best_mxgb_submit = pd.DataFrame(best_mxgb.predict(dtest, ntree_limit=best_mxgb.best_ntree_limit), columns=['phat'])\n",
    "best_mxgb_submit.to_csv('ph_best_mxgb_' + str(datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\") + '.csv'), \n",
    "                        index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mxgb_submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_combiner(training_frame, test_frame, nums):\n",
    "    \n",
    "    \"\"\" Combines numeric features using simple arithmatic operations.\n",
    "    \n",
    "    :param training_frame: Training frame from which to generate features and onto which generated \n",
    "                           feeatures will be cbound.\n",
    "    :param test_frame: Test frame from which to generate features and onto which generated \n",
    "                       feeatures will be cbound.\n",
    "    :param nums: List of original numeric features from which to generate combined features.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    total = len(nums)\n",
    "    \n",
    "    # convert to pandas\n",
    "    train_df = training_frame.as_data_frame()\n",
    "    test_df = test_frame.as_data_frame()\n",
    "    \n",
    "    for i, col_i in enumerate(nums):\n",
    "        \n",
    "        print('Combining: ' + col_i + ' (' + str(i+1) + '/' + str(total) + ') ...')        \n",
    "        \n",
    "        for j, col_j in enumerate(nums):\n",
    "            \n",
    "            # don't repeat (i*j = j*i)\n",
    "            if i < j:\n",
    "                \n",
    "                # convert to pandas\n",
    "                col_i_train_df = train_df[col_i]\n",
    "                col_j_train_df = train_df[col_j]\n",
    "                col_i_test_df = test_df[col_i]\n",
    "                col_j_test_df = test_df[col_j] \n",
    "\n",
    "                # multiply, convert back to h2o\n",
    "                train_df[str(col_i + '|' + col_j)] = col_i_train_df.values*col_j_train_df.values\n",
    "                test_df[str(col_i + '|' + col_j)] = col_i_test_df.values*col_j_test_df.values\n",
    "                \n",
    "    print('Done.')\n",
    "    \n",
    "    # convert back to h2o\n",
    "    \n",
    "    print('Converting to H2OFrame ...')\n",
    "    \n",
    "    training_frame = h2o.H2OFrame(train_df)\n",
    "    training_frame.columns = list(train_df)\n",
    "    test_frame = h2o.H2OFrame(test_df)\n",
    "    test_frame.columns = list(test_df)\n",
    "    \n",
    "    print('Done.')\n",
    "    print()\n",
    "    \n",
    "    # conserve memory \n",
    "    del train_df\n",
    "    del test_df \n",
    "    \n",
    "    return training_frame, test_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H2OFrame(training[x_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model1 = H2ORandomForestEstimator(\n",
    "    ntrees=10000,                    \n",
    "    max_depth=10, \n",
    "    col_sample_rate_per_tree=0.1,\n",
    "    sample_rate=0.8,\n",
    "    stopping_rounds=50,\n",
    "    score_each_iteration=True,\n",
    "    nfolds=3,\n",
    "    keep_cross_validation_predictions=True,\n",
    "    seed=12345)           \n",
    "\n",
    "# start timer \n",
    "rf_tic = time.time()\n",
    "\n",
    "# train rf model\n",
    "rf_model1.train(\n",
    "    y = 'high_priced',\n",
    "    training_frame=H2OFrame(training),\n",
    "    validation_frame=H2OFrame(validation))\n",
    "\n",
    "# print model information\n",
    "rf_toc = time.time() - rf_tic\n",
    "print('Random Forest training completed in %.2f s.' % rf_toc)\n",
    "print(rf_model1)\n",
    "\n",
    "\n",
    "rf_preds1_test = rf_model1.predict(H2OFrame(test))\n",
    "gen_submission(rf_preds1_test) # 0.14574 public leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_submit = rf_model1.predict(H2OFrame(test)).as_data_frame()\n",
    "best_rf_submit.to_csv('best_rf_model_' + str(datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\") + '.csv'), \n",
    "                        index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.100704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.098442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.119991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.065078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.117926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19826</th>\n",
       "      <td>0.124180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19827</th>\n",
       "      <td>0.105582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19828</th>\n",
       "      <td>0.127146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19829</th>\n",
       "      <td>0.056343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19830</th>\n",
       "      <td>0.123059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19831 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        predict\n",
       "0      0.100704\n",
       "1      0.098442\n",
       "2      0.119991\n",
       "3      0.065078\n",
       "4      0.117926\n",
       "...         ...\n",
       "19826  0.124180\n",
       "19827  0.105582\n",
       "19828  0.127146\n",
       "19829  0.056343\n",
       "19830  0.123059\n",
       "\n",
       "[19831 rows x 1 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rf_submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "08b4f54110b6f43b2dd9be32a24414551670316d55ccc2138cb60ebab93e9b17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
